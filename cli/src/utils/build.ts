import * as fs from 'fs';
import * as path from 'path';
import Valist from '@valist/sdk';
import { createBuild, exportBuild, generateDockerfile } from 'reproducible';
import { ValistConfig } from '@valist/sdk/dist/types';
import { stripParentFolderFromPath } from '@valist/sdk/dist/utils';
import { defaultImages, parsePackageJson } from './config';

const createDistJSON = async (valist: Valist, config: any, releaseFiles: any): Promise<string> => {
  const distJson: Record <string, any> = {
    id: `${config.repo}-${config.tag}`,
    version: config.tag,
    name: config.repo,
    platforms: {
    },
  };

  const artifactPlatforms = Object.keys(config.artifacts);
  for (let i = 0; i < artifactPlatforms.length; ++i) {
    const [os, arch] = artifactPlatforms[i].split('/');
    distJson.platforms[os] = {
      archs: {},
    };
    const currentEntry = distJson.platforms[os];
    currentEntry.archs[arch] = {
      // eslint-disable-next-line no-await-in-loop
      cid: (await valist.ipfs.add(releaseFiles[i], { onlyHash: true, cidVersion: 1 })).cid.string,
      path: stripParentFolderFromPath(releaseFiles[i].path, config.out),
    };
  }

  return JSON.stringify(distJson);
};

export const buildRelease = async (valist: Valist, config : ValistConfig):
Promise<(fs.ReadStream | { json: string, path: string | Buffer })[]> => {
  let buildCommand = config.build;
  let output = config.out;

  if (config.type === 'node' && !output) {
    const packageJson = parsePackageJson();
    buildCommand = `${config.build} && npm pack`;
    output = `${packageJson.name}-${packageJson.version}.tgz`;
  }

  if (!output) throw new Error('Output file/directory not defined!');

  // Generate Dockerfile (uses current directory as source)
  generateDockerfile(config.image || defaultImages[config.type], './', buildCommand, config.install);

  await createBuild('valist-build-image');
  await exportBuild('valist-build-image', output);

  // cleanup docker artifacts generated by reproducible
  if (!process.env.KEEP_DOCKERFILE) {
    fs.unlinkSync('Dockerfile.reproducible');
    fs.unlinkSync('Dockerfile.reproducible.dockerignore');
  }

  // read streams for final upload
  const releaseFiles: (fs.ReadStream | { json: string, path: string | Buffer })[] = [];

  if (config.artifacts) {
    const artifacts: string[] = Object.values(config.artifacts);

    // create temp array of stream to allow createDistJSON to
    const tempStreams: fs.ReadStream[] = [];

    for (let i = 0; i < artifacts.length; ++i) {
      releaseFiles.push(fs.createReadStream(path.join(process.cwd(), output, artifacts[i])));
      tempStreams.push(fs.createReadStream(path.join(process.cwd(), output, artifacts[i])));
    }

    // create dist.json from artifacts and push it to releaseFiles
    const distFile = fs.createWriteStream(path.join(output, 'dist.json'));
    distFile.write(await createDistJSON(valist, config, tempStreams));
    distFile.end();

    releaseFiles.push(fs.createReadStream(path.join(process.cwd(), output, 'dist.json')));
  } else if (
    fs.statSync(output).isDirectory()
  ) {
    // if output is a directory, recursively fetch file paths and open readStreams for each
    const getAllFiles = (folder: string) => {
      const files = fs.readdirSync(folder);

      files.forEach((file) => {
        if (fs.statSync(path.join(folder, file)).isDirectory()) {
          getAllFiles(path.join(folder, file));
        } else {
          releaseFiles.push(fs.createReadStream(path.join(process.cwd(), folder, file)));
        }
      });
    };

    getAllFiles(output);
  } else {
    releaseFiles.push(fs.createReadStream(path.join(process.cwd(), output)));
  }

  return releaseFiles;
};
